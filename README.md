# INTC_Stock_Model
Regression model fore predicting INTC stock prices, with other IT stock price data. Report and Analysis are below: 

## Executive Summary:
In the wake of COVID 19 and the recent Ukraine - Russia Crisis, global supply chain operations have been shocked around the world. Industries ranging from agriculture, technology, consumer goods and pharmaceuticals have all faced the consequences of recent supply chain shocks. Market behaviors have reflected this behavior with the S&P 500, NASDAQ and DOW Jones all decreasing at 1.63%, 1.68% and 1.96% respectively within the past 6 months, showing how recent global tensions between NATO, Russia and China have caused decreases in market indexes. In particular, the semiconductor industry has seen some of the largest losses in the past few months in terms of stock prices and overall buy side sentiment.
Sudden increases in semiconductor demand, trade sanctions and an overreliance on semiconductors in the auto industry have all led to recent chip shortages This has led to large amounts of potential revenue being lost from industry giants such as Intel, AMD, and Samsung Electronics. (McKinsey & Company) Further as companies such as Apple and Google, have begun producing their own in-house chips, many of the household semiconductor firms are starting to shift their focus, towards the â€œInternet of Thingsâ€, tech consulting and other forms of business. (Accenture)

The shift in Industry structure combined with recent global events, has led to many unknowns within the semiconductor industry causing widespread speculation in many semiconductor stocks. Because of recent speculating within the industry, it is more important than ever to utilize predictive modeling to try and understand how semiconductor firms have moved in the past year with respect to other firms in the publicly traded markets.

Therefore, in this report I propose a multivariate OLS regression model, that quantifies, stock price movements in a one-year time for INTC (Intel Corporation). Overall, the model reports high utility, with predictive power over 88% and statistical significance as well. Within the model I found that similar players within the semiconductor industry such as TXN, NVDA, SWKS, MU & corporate partners with Intel such as CTXS, are exceptionally well at modeling Intel stock prices. Additionally, though models do exist that have seemingly higher â€˜predictive powerâ€™, they lack the interpretability and other key details.

Below in-depth analysis follows, starting with variables that I added within the dataset, an overview of which companies I aimed my analysis towards, dimensionality reduction techniques, a brief overview of statistical methodologies used, exploratory data analysis, regression results, residual analysis, model validation and correction and finally next steps to improve future models.

## Data Selection & Firm Analysis:
Before going into the specifics of the model itself a few key features about this model and stock market prices in general must be made. First the model that I propose, was created with data ranging from 04/01/21 â€“ 03/31/22, or approximately a 1-year period. Due to the nature of the
  
NYSE, stock price data is not available for a full 365 days. Instead, I report a total of 253 observations per independent variable. Another remark that must be made for this model, is of the nature of stock market prices for prediction. Multiple, theories and hypothesis exist surrounding stock market prices, and the validity of accurately predicting stock prices. Though it is out of scope for the purpose of this project, theories such as Efficient Frontier Hypothesis, Random Walk Theory, Rational Expectations and Prospect theory, all propose unique and various interpretations of stock market pricing and fluctuations. Because of this it is very difficult to model and predict stock market prices, due to the many factors which can affect market prices as described in some of the theories above. For this project, it is important to borrow some ideas from Random Walk Theory, to explain the seemingly unpredictable nature of various price fluctuations, that are observed in the model that I will propose later. I will now give a brief analysis of the variables that were included in the final model, and some of the variables that I decided to add into the initial dataset.

The initial dataset that was given had around 74 total variables all coming from the Information Technology sector of the S&P 500 index. As a quick refresher the S&P 500 is a stock market index that tracks the 500 largest publicly traded companies in the United States. Further the dataset also includes a date section and closing prices for each stock ticker within the dataset. Because of various hypotheses that I drew upon regarding the automotive industry, technology industry and silicon production I also added a total of 20 variables into the initial dataset. Each variable added was sourced from Yahoo Finance from 04/01/21 â€“ 03/31/22. Further for data validation I randomly chose around three variables from the initial dataset and compared them to the yahoo finance data and found that the values were the same.
Regrading why I chose to include each of these variables has to do with how the semiconductor industry has changed over the past year. For companies in the â€œTechâ€ Industry, I decided to include firms such as Dell and Amazon into the dataset. The main reason I chose to include companies like Dell and Amazon has to do with Intelâ€™s customer base. Intel makes a large amount of its revenue from selling their semiconductors and other services to other technology companies. Firms such as Dell create their products using the Intel â€œIâ€ processor line and firms like Amazon have huge corporate partnerships with Intel. Data from silicon production firms was also added into the initial dataset. Because Intelâ€™s primary form of revenue has to do with the manufacturing and selling of semiconductor chips, I hypothesized that silicon production firms may have an interesting relationship with the stock price of Intel. This led to me adding firms such as SUMCO Corporation into, the initial dataset. Finally, I chose to include around 5 publicly traded automotive companies into the model. While researching the topic I found that the automotive industry has a newfound reliance on semiconductor manufacturers. IoT sensors and â€˜connectedâ€™ automobiles have caused the demand for semiconductors to increase substantially for automotive firms, creating a newfound relationship between the semiconductor and automotive industries. Almost all the variables I chose to add into the dataset showed little improvement to overall model performance, except for the automotive firm Tesla. Overall, I chose to base the data which I created the â€˜best fit modelâ€™ on industry trends and actual changes within the semiconductor industry, to increase the modelâ€™s overall interpretability.

## Statistical Methodologies:
In conducting the actual analysis, the main technologies used for modeling are SAS, with exploratory analysis conducted in the python programming language. Before conducting exploratory analysis extensive use of dimensionality reduction techniques such as stepwise, forward, backward, cp and adjusted R-square, were used throughout the report to narrow down potential terms within the model. Reduction techniques were necessary due to the large amount of initial and added variables. (93 total) Further it is important to note, that I iterated multiple times through the dimensionality reduction process, to find â€˜best fitâ€™ terms. After dimensionality reduction was conducted exploratory analysis, went underway on the subset of variables that either stepwise chose or that I handpicked due to hypothesized relationships between Intel and the other tickers. In the exploratory analysis section, I chose to create several time series plots to see how each of the respective â€˜best fitâ€™ terms moved with respect to Intel. After that I researched into each company that was selected as a â€˜best fitâ€™ term and tried to find any recent news or partnerships with Intel that can possibly explain any price fluctuations in the time series plot. Further probability distributions, scatter plots and boxplots were also used to gain further understanding of the relationships, strength, and any possible outliers within the independent variables. During the actual modeling process, I used a multivariate OLS regression model to accurately predict and quantify a relationship between Intel and various other independent variables. After the modeling process ended, I used the variable inflation factor to test for multicollinearity and ended up using the PRESS statistic for model validation and comparison. Finally residual analysis was conducted using the given residual plots and variable transformations were utilized to correct for unequal variances. Like the dimensionality reduction process, each of these techniques were used on an iterative basis, until I found the â€˜best fitâ€™ model.

## Dimensionality Reduction:
As mentioned above throughout the modeling process various variable screening methods were conducted to reduce overall dataset dimensionality. Due to the terms which I added, the dataset had over 90 different possible independent variables. Because of this I first decided to run a stepwise regression on every single variable within the dataset and compare fit it to INTC. After running the stepwise regression, I ended up with around 30 or independent variables that created a so called â€˜best fitâ€™ first order model. However, with this model and various other model I created using stepwise and cp regression, I did not necessarily keep said models, or even consider them as possible models of â€˜best fitâ€™. Instead, I used the variable screening methods as a tool to narrow down which variables I will conduct further research on. Additionally, if for whatever reason a term whoâ€™s voided from the stepwise or other variable screening process, that I thought would alter or affect the â€˜best fitâ€™ model I decided to further research said variable as well.

Though I will not list every single iteration of each stepwise and cp regression that I ran, I was able to narrow down the initial 90 variables into around 30 best fit terms. I also cut off the â€˜best fitâ€™ terms at .95 adjusted R square as an arbitrary point, where an increase in the number parameters led to a smaller increase in overall model performance. In terms of actual model utility this model reported a very high adjusted r square > .95 and had overall model statistical significance. However, there are many issues with this model and similar stepwise model that I ran. First the models created by a stepwise regression often lack any interpretability. 

Though we are analyzing very volatile data, the stepwise regression does not factor in various industries, corporate sponsorships or other factors that can cause stock prices to vary. This causes the models, to make very little sense in terms of a real relationship, and almost seems to be random. Further stepwise regression does not factor in the variable inflation factor, which can cause large amounts of multicollinearity within the stepwise model. Finally, as stepwise crams, more parameters into the model, the adjusted R square will continue to rise due to the nature of how it is calculated. Due to this I believe that my choice to use variable screening methods such as stepwise and cp was justified, allowing me to focus my research efforts on understanding the semiconductor industry and creating a model that is predictive and interpretable.

After conducting initial variable screening methods, I was able to narrow down my total choice variables to about 15 different first order terms. I chose these variables based on the results from my variable screening process and from variables which exhibited a high level of either negative or positive correlation, which I tested for during the dimensionality reduction process. Still though exploring 15 different variables, would be quite time consuming and further some of the variables that were listed seemed to have no interpretable relationship with Intel. Just for a quick example I will cite the stock ticker FISV. FISV or Fiserv Corporation is one of the largest financial technology firms in the world. It is a multinational corporation with over 40,000 employees worldwide and provides payment processing and other fintech services to firms around the globe. However, during the research process, I found no corporate sponsorships or connection between Intel and Fiserv. Because of this I decided to take FISV out of my â€˜poolâ€™ of independent variables that I decided needed further investigation. Several other firms received similar treatment to FISV such as PYPL (PayPal) and V (Visa). Though this process did decrease the number of variables within the dataset and potential powerful predictor variables, it was necessary to keep my potential first order model interpretable.

From these 15 first order terms I ran several other iterations of stepwise with the 15 â€˜best fit terms. For the terms that I took out I replaced them with other semiconductor companies and found that there does seem to be a relationship with competing semiconductor companies and Intel. From here I was able to narrow down my â€˜best fitâ€™ terms to TSLA, NVDA, TXN, MU SWKS, ORCL, QCOM, AMD, CTXS. From here I then narrowed down these terms to the following based of the results of a stepwise and cp regression; TSLA (Tesla), NVDA (Nvidia), TXN (Texas Instruments), MU (Micron Technology) SWKS (Skyworks Solutions) and CTXS (Citrix Systems). Further each of these terms were either related to the semiconductor industry or had recent news with Intel, which may explain why these terms have a strong relationship with Intelâ€™s stock price. For example, Citrix Systems has been a longstanding partner with Intel for the past few years and both companies have large corporate partnerships and deals with one another. (Citrix) Further Tesla recently announced that they are cutting ties with Intel and have announced a partnership with AMD for semiconductors, which can be a reason why there is a strong negative correlation (r>-0.70) between the two. From here I concluded the dimensionality reduction process, and concluded that the following six terms; TSLA, NVDA, TXN, MU SWKS and CTXS, should be further explored within an exploratory data analysis to gain more insight into potential â€˜best fitâ€™ models.

On a separate note, within the appendix, there will be a section with the stepwise and cp models that I iterated through in this section.

## Exploratory Data Analysis:
After narrowing down the 90 initial variables down too 6, I begun an in-depth exploratory analysis and market research into each 6 of the variables. Before even going into the EDA, I must first briefly describe each of the potential â€˜best fitâ€™ model variables. TSLA or Tesla Inc, is an automotive company most known for its digitally â€œconnectedâ€ all electric vehicles. In recent years Tesla has seen extraordinary gains within its stock price, increasing by over 31.33% in the past year. (Google Finance) Like I mentioned in the dimensionality reduction section, Tesla has recently announced that Intel will no longer be the main supplier of semiconductors for the interior of their vehicles. Instead, they will purchase chips from AMD, one of Intelâ€™s largest market competitors. Though it is impossible to imply causation, there seems to be a strong negative relationship between Intel and Tesla, reporting a correlation coefficient of -0.77. Nvidia is another firm that I decided continue analysis and research on. Nvidia is another one of Intelâ€™s largest competitors in the chip and manufacturing industry. Though Nvidia mostly specializes in GPUâ€™s, the two companies are still fierce competitors since they have several overlapping products. Correlation analysis between the two companies shows a negative relationship between the two, which is further supported with a correlation coefficient of -0.66. Texas instruments is another one of Intelâ€™s major competitors that produces semiconductors, chips and most notably the famous T-I calculator series. Further, MU (Micron Technology) and SWKS (Skyworks Solutions) are major semiconductor players that also compete with Intel for market share. Micron Technology is also known for data storage and specifically in developing RAM, flash storage and flash drive products. Skyworks specializes in creating semiconductors for use in mobile phones and radio frequency devices. Finally, CTXS (Citrix Systems) is one of Intelâ€™s largest corporate partners and the two firms have a collaborative relationship with one another. (Reuters) Further here is a table of the correlation values between INTC and the respective variables that I mention above:

<img width="437" alt="Screen Shot 2022-04-29 at 9 29 05 PM" src="https://user-images.githubusercontent.com/88412646/166090955-11e7c4ae-d6a2-40e1-80cb-252c1fdb826c.png">

After clearly understanding each of the variables and potential relationships with Intel, we can now move towards the actual exploratory analysis of each of the potential predictor variables. Since we are dealing with time series data, the first thing I did was plot a basic line plot with time as the x-axis and prices on the y-axis. Further I standardized the price part of the plot so I can superimpose each of the tickers price movement data with respect to time. Doing this reveals that there are clear relationships between each of the independent variables and Intel. For example, the positive relationships shown between Intel, SWKS and CTXS are supported by the time series plots as the price of each variable moves in relative tandem with one another. Further stocks with a negative correlation such as TSLA and NVDA, move in an inverse relationship with Intel, which shows a great visual representation of the stock price movements. For variables like TXN and MU, there does not seem to be strong movement between the variables, though TXN seems to somewhat move with Intel price changes. 

After plotting each of the variables against Intel in time series plots, I decided to create probability distributions for each variable to better understand the shape, skew, and overall distribution of the independent variables. Before plotting each variable, I first transformed the data set, using z-scores. After normalizing the dataset, I plotted each independent variable. Out of all the independent variables, none seem to follow a perfect normal distribution. TSLA has high skew to the right and high amounts of spread to the upper bound indicating the volatile nature of TSLA stock prices. Further the right skew shows that during the past year there were periods in which TSLA stock prices increased substantially. NVDA also reports slight skew to the right, implying periods of stock growth or potential peaks in the stockâ€™s prices within the past year. SWKS reports a relatively normal distribution with slight skew to the left. This indicates that within the year SWKS had prices that were predictable with slight variation towards the lower bound, implying periods of price decrease. TXN has a very strong skew to the left, indicating that prices decreased substantially within TXN stock in the past year. Further this shows high amounts of variation within the stock and suggests the need for a variable transformation. MU shows a slight skew to the right, indicating price increases within the past year. CTXS shows the most amount of variation out of all the stocks with a strong skew to the right. The high variation within CTXS indicates that prices for the stock in the past year were challenging to predict and measure. Because of the high amounts of right skew shown by CTXS I decided to log transform the variable, which did help reduce variation. Further I also cubed TXN to correct for the left skew which reduced the variation within the variable. Within Intel the prices showed slight skew to the right, implying high amounts of variation within the stockâ€™s prices within the past year. Overall, each of the variables (dependent included) had amounts of variation within the prices. However, this is excepted as the past year has shown large amounts of volatility within the semiconductor industry.

After analyzing each of the given probability distributions, I decided to create scatter plots for each of the independent variables to visualize each variables relationship with Intel. TSLA and NVDA both show negative relationship between Intel. Additionally, the price points of both TSLA and NVDA seem to follow similar clusters of data, with one main cluster and two smaller clusters, which could potentially be marked as anomalies / outliers. SWKS has a very strong positive relationship with Intel, and it seems it will be one of the stronger predictors for Intel stock based off this graph and the time series plots mentioned earlier. Additionally, the datapoints seem to have a small cluster to the far right which can potentially be outlier prices for both Intel and SWKS. TXN also seems to have somewhat of a positive linear relationship with Intel, but it is not as strong as SWKS. TXN also seems to have various clusters which most of the data points seem to be around. This can explain why the relationship between TXN, and Intel is not as strong as SWKS. MU does not seem to have any relationship with Intel at all, with most of the data points being clustered in a circle like form, with small outliers to the upper right. Finally, CTXS seems to have a positive relationship with Intel. However there does appear to be a set of data points which almost appear vertical in nature.

Next, I decided to create boxplots for each of the independent variables, to detect any other outliers within each respective variable. Analyzing each of the boxplots, it is immediately noticeable that large amounts of variation lie within TXN and CTXS. TXN shows many outliers to the left, confirming the left skew that was seen in the probability distributions for TXN.
Additionally, in CTXS large amounts of outliers also exist to the upper right bound of the boxplot, also confirming the results found in the probability distribution. Besides those two TSLA, NVDA, SWKS and MU do not necessarily report any outliers, and all reflect the distribution seen in each variableâ€™s respective probability distribution. Finally, I created a heatmap to visualize the correlations between each of the independent variables and Intel.

## Multivariate Regression Analysis Results:
After analyzing each of the independent variables I began the actual modeling process with the 6 â€˜best fitâ€™ variables. Before creating any models, I first wanted to see if there are any possible interaction terms for the given variables. To check I decided to rerun a correlation analysis, to see if there are any relationships between my independent variableâ€™s that can possibly be explained with an interaction term. I found that there appears to be a relationship between TSLA and NVDA, NVDA and CTXS, NVDA and SWKS, CTXS and TSLA and CTXS and SWKS. After finding given relationships I decided to create effect plots for each of the combinations to test for the interaction. Based on the interaction plots each of the given combinations above seemed to indicate some interaction between the two terms. Additionally, I tested a quadratic term for TXN, to see if there would be any improvement in the model performance based of the second order term. However, I am very skeptical about including the quadratic term because there seems to be a stronger linear relationship then curvilinear relationship based off the scatterplot. After creating each of the interaction terms and second order term I decided to create a first order model with all my independent variables plus the newly created terms. For sakes of saving space, I not include the regression equation in this section of the report, however results can be found in the appendix. To evaluate our â€œcompleteâ€ first order model with interaction, we must first write out the null and alternate hypothesis for this problem. Additionally letâ€™s assume alpha = 0.05 for every single model in this report.
ğ»:ğ›½+â‹¯+ğ›½=0 01 12
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0

Looking over the ANOVA table this model reports overall statistical significance. The p value is > 0.001 meaning that we can claim the overall model has statistical significance. Further when evaluating the model parameters, each parameter reports statistical significance with p values < 0.05. This allows us to reject the null hypothesis and conclude that the overall model and parameter estimates are all statistically significant at an alpha level of 0.05. Further evaluating the model, we report very low coefficient of variation at only 2.538 percent, with a root MSE of 1.359. Additionally, the adjusted R- square for the model is very good with 90.43% of the variation in INTC being explained by NVDA, TXN, MU, CTXS, SWKS, TSLA and the various interaction terms that I proposed above. However due to the nature of interaction terms when analyzed with the variance inflation factor it appears that multicollinearity becomes a huge issue within the model, with some VIFâ€™s approaching values > 1000. Because of this I was weary when including interaction terms in my final model, however I will discuss this later in my report in the model comparison section. Overall, this model seems to be quite good at predicting INTC, however there are several key factors about this model that make it quite poor. First, the high number of parameters within the model make it very hard to practically interpret. Additionally, we may face issues with overfitting, as this model is highly biased towards the data that it was fit on.

After running this model, I ran another stepwise regression, to gain a better idea of which variables best fit too INTC. Running the stepwise showed that interaction between CTXS and SWKS is the best fit term to the model. I found this very interesting as it suggests that the interaction between CTXS and SWKS may help in predicting INTC stock prices. However, the stepwise did not include the original quotients of the interaction term, which must be noted when using the product of CTXS and SWKS. For the final iteration of the stepwise regression, the algorithm suggests NVDA, TSLA, TXN, MU, SWKS, CTXS, NVDA * TSLA, NVDA * CTXS and CTXS * SWKS, be used for a first order model of best fit. (For the actual regression equation check appendix below) After analyzing the results of the stepwise I decided to stop at the third â€œstepâ€ of the algorithm but to also correct for the algorithms inaccuracy of not including the necessary first order terms with the interaction. To evaluate this model, we must first write the null and alternate hypotheses:
ğ»:ğ›½+â‹¯+ğ›½=0 015
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
Further we can write the regression equation for the following problem as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ¼ğ‘ğ‘‡ğ¶ = âˆ’46.38 + 0.23 (ğ‘‡ğ‘‹ğ‘) + 0.14 (ğ‘€ğ‘ˆ) + 0.38(ğ¶ğ‘‡ğ‘‹ğ‘†) + 0.14 (ğ‘†ğ‘Šğ¾ğ‘†) âˆ’ 0.001 (ğ¶ğ‘‡ğ‘‹ğ‘† âˆ— ğ‘†ğ‘Šğ¾ğ‘†)

Given the regression equation we can interpret the model as follows. For each price increase in TXN the predicted price of INTC increases by 0.23, for each increase in MU the predicted price of INTC increases by 0.14, for each increase in CTXS the predicted price of INTC increases by 0.38, for each additional increase in SWKS, predicted price of INTC increases by 0.14 and the product of CTXS and SWKS causes predicted prices to decrease by 0.001.
Checking the F-Statistic in the ANOVA table the overall model reports statistical significance. We report an F-Value of 350.76 and a corresponding p value < 0.001. Further checking the parameter estimates confirms this, with every single term showing statistical significance (p < 0.05), except for CTXS * SWKS, which reports a p value of 0.09, meaning that in this model the interaction effect is not statistically significant. Still though we have enough significant parameter estimates to conclude that the model is statistically significant at an alpha level of 0.05. This allows us to reject the null hypothesis and claim that TXN, MU, CTXS and SWKS all have a statistically significant relationship in modeling INTC. Further analyzing overall model utility there are several positive indicators in favor of this model. First the adjusted R square for this model is relatively high at around .8741. This means that around 87.41% of the variation in INTC can be explained by TXN, MU, CTXS, SWKS and CTXS * SWKS. Additionally, the model reports relatively low root MSE of only 1.55 and a coefficient of variation of 2.9%. Overall, when analyzing the ANOVA table and the parameter estimates this model is quite good for predicting INTC. However due to the insignificant interaction term, I decided to iterate through several other models which I will briefly analyze.

After realizing the interaction term had little effect on the model, I decided to add another parameter estimate to the model, in hopes that it would make the interaction term statistically significant once more. Additionally, I chose the added term based on â€˜step 4â€™ of the stepwise regression algorithm, which ended up being NVDA. Again, to test the model, we need to write out the null and alternate hypotheses once again for the problem (assume alpha = 0.05).
ğ»:ğ›½+â‹¯+ğ›½=0 016
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
After fitting the model to the data, we can write the regression equation for the following problem as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ¼ğ‘ğ‘‡ğ¶ = âˆ’15.70 âˆ’ 0.03 (ğ‘ğ‘‰ğ·ğ´) + 0.26 (ğ‘‡ğ‘‹ğ‘) + 0.17(ğ‘€ğ‘ˆ) + 0.08 (ğ¶ğ‘‡ğ‘‹ğ‘†) âˆ’ 0.001 (ğ‘†ğ‘Šğ¾ğ‘†) + 0.0002(ğ¶ğ‘‡ğ‘‹ğ‘† âˆ— ğ‘†ğ‘Šğ¾ğ‘†)

Given the regression equation we can interpret the model as follows. For each price increase in TXN the predicted price of INTC increases by 0.26, for each increase in MU the predicted price of INTC increases by 0.17, for each increase in CTXS the predicted price of INTC increases by 0.08, for each additional increase of NVDA predicted INTC price decreases by 0.03, for each additional increase in SWKS, predicted price of INTC decreases by 0.001 and the product of CTXS and SWKS causes predicted prices to increase by 0.00002.

After iterating through the model, we can check the ANOVA table first for overall model utility. Our model reports a very large f statistic that is greater than 300 and reports a corresponding p value < 0.01. This allows us to conclude that the overall model is statistically significant at an alpha level of 0.05. Next, we can check the model parameter estimates to test for parameter significance. In the parameter estimates NVDA, TXN and MU all report statistical significance with p values < 0.01. However, the addition of NVDA effected the model significantly and led to CTXS, SWKS and CTXS * SWKS all becoming insignificant with p values substantially greater than > 0.05. Even though three of the terms in the model were insignificant we can still reject the null hypothesis and claim that at least three of the parameters within our model are statistically significant. After analyzing the model for statistical significance, we can check the adjusted R square, root MSE and coefficient of variation for overall model utility. Our models ANOVA table reports and adjusted R-square of 0.8880, which means that 88.88% of the variation in INTC can be explained by NVDA, TXN, MU, CTXS, SWKS and CTXS * SWKS. This is very good in terms of variation, and it suggests that only 11.12% of the variation is unaccounted for. Additionally, the root MSE for the model is quite low at 1.467 and the coefficient of variation is 2.746 percent. Overall, this model is quite good at modeling and explaining INTC price changes and fluctuations. The model does lack in several factors as the interaction terms and corresponding quotients all become insignificant in this model. Additionally, the addition of more parameters does affect the overall power of the model and increases the variation within the predicted values.
Finally, I iterated through one more model, this time though leaving the interaction term out altogether and only including NVDA, TXN MU, CTXS and SWKS. I decided to do so to account for the insignificance that the interaction term has reported for the last two models I

iterated through. Once again, we must start with writing out the null and alternate hypothesis for our regression problem.
ğ»:ğ›½+â‹¯+ğ›½=0 015
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
Once we write out the null and alternate hypothesis, we can write out the regression equation as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ¼ğ‘ğ‘‡ğ¶ = âˆ’20.70 âˆ’ 0.03 (ğ‘ğ‘‰ğ·ğ´) + 0.26 (ğ‘‡ğ‘‹ğ‘) + 0.17(ğ‘€ğ‘ˆ) + 0.12 (ğ¶ğ‘‡ğ‘‹ğ‘†) + 0.02(ğ‘†ğ‘Šğ¾ğ‘†)
Given the regression equation we can interpret the model as follows. For each price increase in NVDA the predicted price of INTC decreases by 0.03, for each increase in TXN the predicted price of INTC increases by 0.26, for each increase in MU the predicted price of INTC increases by 0.17, for each additional increase of CTXS predicted INTC price increases by 0.12 and for each additional increases in SWKS, the predicted price of INTC increases by 0.02.
After interpreting the regression equation for the given model, we can evaluate the ANOVA table and parameter estimates to test for statistical significance. Within the ANOVA table we report a p value < 0.0001. This means that at an alpha level of 0.05 the overall model is statistically significant. Because of this we can continue testing our model and further dive into the parameter estimates of the model. Looking at the parameter estimates each of the values are statistically significant, with each parameter estimate reporting a p value < 0.05. This allows us to reject the null hypothesis and claim that each of the parameter estimates within our model are significant for modeling and predicting INTC prices. From here we can then inspect the ANOVA table to test overall model fit and utility. Our model reports a high adjusted R square value of 0.8884. This means that around 88.84% of the variation in INTC can be explained by NVDA, TXN, MU, CTXS and SWKS. In terms of model fit this is a very significant result which implies that only 11.16% of the variation in INTC is unaccounted for by our model. Additionally, the model reports a root MSE of 1.464 and a coefficient of variation of 2.74%, which both indicate low variance within the predicted values. From our initial model analysis, it seems that this model is the best fit, it reports significant parameter estimates, has low variation in predicted y values and has overall good fit. Additionally, when compared to previous models the number of parameters for this model are relatively low. Also, this model is a subset of our first order model with interaction terms, implying that we may have a parsimonious model. In the next section I will tweak the three models that we have narrowed down in this section of analysis. Additionally, I will conduct residual analysis for each of the three models, before determining the â€˜best fitâ€™ model.

## Residual Analysis & Model Correction:
From the exploratory data analysis conducted earlier in this report CTXS and TXN both need a variable transformation to correct for the high amounts of variation within each respective variables price data. For TXN I decided to cube the variable to correct for the large amounts of skew to the left. Doing so lowered the amount of variation within the variable substantially. For
 
CTXS I decided to log transform the variable to correct for the skew to the right. Doing so also lowered the amount of variation in the variable, which will hopefully decrease variation across all models. Additionally, I needed to transform the interaction term between CTXS and SWKS to account for the log transformation of CTXS. With the transformed TXN and CTXS variables I decided to iterate each of the models a second time with the corrected terms. In the table below I summarize the results of the variable transformation, in terms of effects on the root MSE:

<img width="438" alt="Screen Shot 2022-04-29 at 9 31 05 PM" src="https://user-images.githubusercontent.com/88412646/166091008-68c4101e-6ab1-453b-b5a1-986c8ed365eb.png">

The proposed variable transformations increased the amount of variation in all three models of the models. I hypothesize this can be since to see efficacy in variable transformation efforts all the other terms must be transformed as well. So, to test this, I decided to transform all the given variables, as they each did show some form of skew in the exploratory data analysis. Results are shown in the table below:

<img width="436" alt="Screen Shot 2022-04-29 at 9 31 24 PM" src="https://user-images.githubusercontent.com/88412646/166091026-884d2d8d-0f56-4c60-bdf6-57398561877e.png">

From the results log transforming all the variables
during this process, I noted something interesting.
INTC, I noticed that almost all the residual charts,
binomial or multiplicative in nature. For example,
residuals and variableâ€™s such as TXN show similar patterns. Because of this I decided to transform the dependent variable (INTC) instead by applying a log transformation to hopefully correct for the unequal variances. Taking a log transformation of INTC decreased the root MSE within all three of the models substantially. After the transformation model 1 reports a root MSE of 0.029, model 2 reports a root MSE of 0.02742 and model 3 reports a root MSE of 0.02740. Additionally, each of the residual regressor plots have much smaller intervals in terms of variance. Overall, this transformation was a large success in correcting unequal variance and fixed the issue that was occurring within each of the proposed models. After fixing the issue with unequal variances, I analyzed the residuals in each respective model mostly looking at the residual regressor plots, root MSE and Cookâ€™s D to find potential outliers within each of the models. What I found is that across all three models, the residual regressors for each have relatively similar patterns for each variable. Though the variable transformation for INTC did fix
is not the right approach as well. However, Within the actual residuals by regressor for show some form of unequal variance, either MU shows a fanning out nature within the
the issue of high root MSE, there still exists similar patterns that look quite like the ones before the transformation was applied. Additionally model 3 had the lowest root MSE out of all the iterations. One area where model 3 lacked in was the Cookâ€™s Distance plots. Model 3 had multiple observations that were significantly above the 0.02 threshold of the Cookâ€™s D plots. Further both model 1 and model 2 only had 1 observation that seemed to be significantly above the 0.02 threshold of Cookâ€™s D. I chose not to remove the observations that appeared as outliers, because I believe that they are legitimate points that should be considered in our model. As mentioned in the data section of this report, stock prices are bound to have various outliers, due to seemingly stochastic nature of stock markets. In the next section of this report, I will compare each of the three models I propose above with one another, using the press statistic as the main method of validation. Additionally, I will summarize each of the three models and determine the model of best fit.

## Model Comparison & Validation

After creating each of the three models above, correcting for unequal variances and analyzing each modelsâ€™ residuals with respect to one another. I decided to use the press statistic to aid in the final portions of this report to help determine which model is of â€˜best fitâ€™. Additionally in this section of the report I will consider other factors such as multicollinearity, model utility, statistical significance and interpretability when deciding the best fit model. The press statistic was chosen to aid in the validation / model comparison process, because of how it is calculated. Since the press statistic is calculated by holding out one point at a time and then attempts to compute the predicted value, it essentially acts as a measure of seeing how well a model will make new predictions. For model one we report a press statistic of 0.22, for model two a press statistic of 0.1973 and for model 3 a press statistic of 0.1954. Based off the press statistics alone it is clear that model 3 is superior since we can predict values of INTC with more accuracy as compared to model 1 and model 2. (Keep in mind that the press statistic was computed with the corrected INTC value for each model) Additionally model 3 does not suffer from multicollinearity at all, with each of the variables having VIF scores < 6, indicating that the predictor variables are independent from one another. In comparison, both model 1 and model 2 report extremely high VIF values due to the interaction terms applied to both models. Finally, when checking the model for normality within the residuals, the residual histogram follows a normal distribution and the QQ plot also fits the line quite well, implying that our model follows the normality assumption when conducting OLS regression.

Because model 3 has a high adjusted R square, the lowest root MSE, the lowest press statistic, the highest amount of statistical significance in the overall model and in each of the parameters, and most importantly since the model is interpretable and makes â€˜senseâ€™ in terms on Intel and the semiconductor industry, I believe that it is the best fit model for predicting Intel stock prices. Though the model does give up higher values of adjusted R square, I argue that adding unrelated and nonsensical terms to a statistical model does very little in improving our understanding of stock prices movements for Intel.

## Conclusions and Next Steps
These past two years have revealed how unpredictable markets and the economy truly are. Seemingly random global events such as COVID 19 can cause whole economies to change in ways that no analyst or algorithm could ever predict. Further, supply and demand shocks have caused massive shortages in labor, consumer goods and other critical materials that were once in abundant in a pre-COVID world. Because of this it is more important now than ever that we can understand what causes fluctuations and changes in each sector of the economy. Doing so allows businesses to make data driven decisions that can potentially benefit the lives of not only the consumers it serves but most importantly those, who work for the firm. Strong bottom lines, aided by data driven decisions, allow businesses to continuously pay their employees and expand job opportunities for current and potential employees.

Though the model that I propose only focuses on the semiconductor industry and specifically Intel, itâ€™s overall interpretability, low predictive variation and good model fit imply that with proper analytical tools it is possible to gain a better understanding of the seemingly random nature of stock market prices and fluctuations. However, the model is not perfect and has some general flaws that I believe must be addressed.

Since we are dealing with time series data, it would have been smart to test for seasonality within the model. Though it is out of the scope for this project some sort of ARIMA or other time series model may predict INTC prices with a higher degree of accuracy than the current model I propose. Further I would have preferred to increase the timeline of the model substantially, as more data could strengthen the effects of the central limit theorem, causing less variation in each independent variable and improving the predictive power of the model itself. Additionally various other data sources and financial instruments are omitted from this model which all could have helped predict INTC ticker prices. For example, the bond market may prove to be an important predictor of INTC prices, further foreign suppliers of Silicon could have been included within the model or even raw commodity prices. Finally having a live stream of data, or at the very least a data source that updates once at the end of each trading day, could allow us to evaluate the Models performance in real time and tweak the model ad hoc. This idea can be implemented relatively easily using the python programming language and various free APIâ€™s available through yahoo finance. In the future I hope to incorporate some if not all the methods above to increase the overall predictive power of the model which I propose today.

## Graphs

<img width="439" alt="Screen Shot 2022-04-29 at 9 32 38 PM" src="https://user-images.githubusercontent.com/88412646/166091067-a6839f68-626e-49ea-9dc8-8906f443e879.png">

<img width="442" alt="Screen Shot 2022-04-29 at 9 32 58 PM" src="https://user-images.githubusercontent.com/88412646/166091077-2cd46db9-8373-4e8c-a2a4-425ebe0203e4.png">

<img width="446" alt="Screen Shot 2022-04-29 at 9 33 08 PM" src="https://user-images.githubusercontent.com/88412646/166091085-08c2f585-5580-4181-81d7-210546fa9300.png">

<img width="450" alt="Screen Shot 2022-04-29 at 9 33 25 PM" src="https://user-images.githubusercontent.com/88412646/166091089-1bb4154b-25d2-4ed0-8e36-61a5af14e182.png">

<img width="462" alt="Screen Shot 2022-04-29 at 9 33 34 PM" src="https://user-images.githubusercontent.com/88412646/166091097-ea31de33-5b97-436c-9f18-bc3b473d90bf.png">

<img width="492" alt="Screen Shot 2022-04-29 at 9 33 52 PM" src="https://user-images.githubusercontent.com/88412646/166091110-f8590966-9dc3-4cab-bdb2-57a29ee9a790.png">

<img width="495" alt="Screen Shot 2022-04-29 at 9 34 03 PM" src="https://user-images.githubusercontent.com/88412646/166091120-ecf1d544-b9b5-48b7-9f33-eba500beac88.png">

<img width="506" alt="Screen Shot 2022-04-29 at 9 34 21 PM" src="https://user-images.githubusercontent.com/88412646/166091128-8da3d3a5-972b-4e6a-8785-4dd1586255eb.png">

<img width="503" alt="Screen Shot 2022-04-29 at 9 34 31 PM" src="https://user-images.githubusercontent.com/88412646/166091135-5bc4a6c6-0484-41c6-9662-075aede3203b.png">

<img width="502" alt="Screen Shot 2022-04-29 at 9 34 43 PM" src="https://user-images.githubusercontent.com/88412646/166091140-e7cf9d38-f07a-45c2-a7f9-bbe3747ce050.png">

<img width="502" alt="Screen Shot 2022-04-29 at 9 34 56 PM" src="https://user-images.githubusercontent.com/88412646/166091151-6b224da0-7355-41af-a8c2-b1d49a161c9b.png">
